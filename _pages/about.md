---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!---
<p align="center">
  <img src="https://github.com/peterbhase/peterbhase.github.io/blob/master/images/s2.jpg?raw=True" alt="Photo" style="width: 300px;"/> 
</p>
-->

## About Me

I am a software engineer in Google Cloud AI working on text generation factuality of Gemini. I obtained my PhD degree from Duke University in 2023, advised by Professor [Ricardo Henao](https://ece.duke.edu/faculty/ricardo-henao). My PhD research lies in the area of low-resource training with pretrained deep learning models, for the tasks of text generation, text/image retrieval, natural language understanding, medical data analysis, etc. Previously, I interned in Adobe (2023) working on mitigating hallucination in text generation with large language models. I also interned in Adobe (2021) on continual few-shot learning and Amazon (2020) on knowledge distillation.



## News
* 2024 - Join Google Cloud AI working on Gemini
* 2023 - Review for ARR, EMNLP, BMCV, AISTATS, SIGIR, UAI and AAAI
* 2023 - Research Intern in Adobe Research on mitigating hallucination in text generation
* 2023 - One Paper accepted by NeurIps
* 2023 - One paper accepted by Findings of ACL
* 2023 - Invited talk in Fidelity Investments on "Efficient Low-Resource Training with Pretrained Language Models"
* 2023 - One paper accepted by AAAI
* 2023 - One paper accepted by AISTATS
* 2022 - Review for AAAI, ARR, BMVC
* 2022 - Research Intern at Adobe Research
* 2022 - One paper accepted by Findings of EMNLP
* 2022 - One paper accepted by ACL
* 2021 - Review for AAAI
* 2021 - One paper accepted by EMNLP
* 2020 - Applied Scientist Intern at Amazon Alexa
* 2020 - One paper accepted by Findings of EMNLP

## Selceted Publications

1. **Wang R**, et al. "Personalized Federated Learning for Text Classification with Gradient-Free Prompt Tuning" (Findings of NAACL 2024)

2. Huang C, **Wang R**, Xie, K, Yu T, Yao L. Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive Model-Aware Approach. arXiv preprint arXiv:2404.03514.

3. Xia Y, **Wang R**, Liu X, Li M, Yu T, Chen X, McAuley J, Li S. Beyond chain-of-thought: A survey of chain-of-x paradigms for llms. arXiv preprint arXiv:2404.15676.

4. Wu J, Yu T, **Wang R**, Song Z, Zhang R, Zhao H, Lu C, Li S, Henao R. Infoprompt: Information-theoretic soft prompt tuning for natural language understanding. Advances in Neural Information Processing Systems, 36.

5. **Wang R**, Yu T, Wu J, Zhao H, Kim S, Zhang R, Mitra S, Henao R. "Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets." Findings of the Association for Computational Linguistics: ACL 2023. 

6. Wu J\*, **Wang R**\*, Zhao H, Zhang R, Lu C, Li S, Henao R. "Few-Shot Composition Learning for Image Retrieval with Prompt Tuning." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 4. 2023.

7. **Wang R**, Cheng P, Henao R. "Toward Fairness in Text Generation via Mutual Information Minimization based on Importance Sampling." International Conference on Artificial Intelligence and Statistics (AISTATS). PMLR, 2023.

8. **Wang R**, Yu T, Zhao H, Kim S, Mitra S, Zhang R, Henao R.  "Few-Shot Class-Incremental Learning for Named Entity Recognition." Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL). 2022.

9. Wu J\*, **Wang R**\*, Yu T, Zhang R, Zhao H, Li S, Henao R, Nenkova A. "Context-aware Information-theoretic Causal De-biasing for Interactive Sequence Labeling." Findings of the Association for Computational Linguistics: EMNLP 2022.
    
10. **Wang R**, Henao R. "Unsupervised Paraphrasing Consistency Training for Low Resource Named Entity Recognition." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2021.

11. Dov D, Assaad S, Si S, **Wang R**, Xu H, Kovalsky SZ, Bell J, Range DE, Cohen J, Henao R, Carin L. "Affinitention Nets: Kernel Perspective on Attention Architectures for Set Classification with Applications to Medical Text and Images." Proceedings of the Conference on Health, Inference, and Learning. 2021.

12. **Wang R**, Si S, Wang G, Zhang L, Carin L, Henao R. "Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning." Findings of the Association for Computational Linguistics: EMNLP 2020.

13. Si S, **Wang R**, Wosik J, Zhang H, Dov D, Wang G, Carin L. "Students Need More Attention: Bert-Based Attention Model for Small Data with Application to Automatic Patient Message Triage." Machine Learning for Healthcare Conference. PMLR, 2020.

14. **Wang R**, Wang G, Henao R. "Discriminative Clustering for Robust Unsupervised Domain Adaptation." arXiv preprint arXiv:1905.13331 (2019).



